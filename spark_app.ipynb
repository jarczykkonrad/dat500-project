{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master(\"yarn\").config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.cores\", 4) \\\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=[\"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0\"]).getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.OFF)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.OFF)\n",
    "\n",
    "sc.setLogLevel(\"OFF\")\n",
    "sc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = \"hdfs:/project/books_demo3/part*\"\n",
    "books_ratings = \"hdfs:/project/ratings_demo3/part*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_books = \"Title STRING, Description STRING, Authors STRING, Image STRING, PreviewLink STRING, Publisher STRING, PublishedDate DATE, InfoLink STRING, Categories STRING, RatingsCount INT\"\n",
    "schema_ratings = \"ID STRING, Title STRING, Price INT, User_id STRING, User_name STRING, Helpfulness STRING, Score FLOAT, Timestamp INT, Summary STRING, Review STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema_books) \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .load(books_data)\n",
    "df_ratings = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema_ratings) \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .load(books_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errasing not needed columns\n",
    "df_books = df_books.drop(\"PublishedDate\",\"Description\",\"Authors\", \"Image\", \"PreviewLink\", \"Publisher\", \"PublishedDate\",\"InfoLink\",\"RatingsCount\")\n",
    "df_ratings = df_ratings.drop(\"ID\", \"Price\",\"User_id\",\"User_name\",\"helpfulness\", \"Timestamp\",\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = df_books.repartition(150)\n",
    "df_ratings = df_ratings.repartition(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning null values\n",
    "df_books = df_books.na.drop(subset=[\"Categories\", \"Title\"])\n",
    "df_ratings = df_ratings.na.drop(subset=[\"Title\", \"Score\", \"Review\"])\n",
    "first_row = df_ratings.limit(1)\n",
    "df_ratings = df_ratings.subtract(first_row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"Review\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "token = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")\n",
    "\n",
    "vivekn = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setOutputCol(\"result_sentiment\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"result_sentiment\"]) \\\n",
    "    .setOutputCols([\"Sentiment\"]) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])\n",
    "\n",
    "pipelineModel = pipeline.fit(df_ratings)\n",
    "df_ratings = pipelineModel.transform(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.write \\\n",
    "    .mode(\"overwrite\").format(\"delta\").save(\"/tmp/books\")\n",
    "df_ratings \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").save(\"/tmp/ratings1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing positive/negative with value\n",
    "df_ratings = df_ratings.withColumn('Sentiment', F.when(df_ratings.Sentiment[0] == \"positive\", 1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.repartition(\"Title\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count between 50 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, calculate the count for each title\n",
    "df_count = df_ratings.groupBy(\"Title\").agg(F.count(\"*\").alias(\"Count\"))\n",
    "df_ratings.unpersist()\n",
    "\n",
    "# Filter the DataFrame based on the count\n",
    "df_count_filtered = df_count.filter(df_count[\"Count\"] > 50).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using brodacast as optimization\n",
    "df_ratings_filtered = df_ratings.join(F.broadcast(df_count_filtered), \"Title\")\n",
    "\n",
    "# Now, calculate the average scores and sentiments\n",
    "\n",
    "df_ratings = df_ratings_filtered.groupBy(\"Title\", \"Count\").agg(\n",
    "    F.mean(\"Score\").alias(\"Scores\"),\n",
    "    F.mean(\"Sentiment\").alias(\"Sentiments\")\n",
    ")\n",
    "\n",
    "df_count_filtered.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").save(\"/tmp/ratings_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper limit for amount of reviews\n",
    "df_ratings_updated = df_ratings.withColumn('Count', F.when(F.col(\"Count\") > 100, 100).otherwise(F.col(\"Count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_updated \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").save(\"/tmp/ratings_count_updated\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging of delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the Delta tables\n",
    "df_ratings_temp = spark.read.format(\"delta\").load(\"/tmp/ratings_count\")\n",
    "df_ratings_updated_temp = spark.read.format(\"delta\").load(\"/tmp/ratings_count_updated\")\n",
    "\n",
    "# Read the data back as Delta tables\n",
    "delta_table_ratings = DeltaTable.forPath(spark, \"/tmp/ratings_count\")\n",
    "\n",
    "# Perform the MERGE INTO operation\n",
    "delta_table_ratings.alias(\"ratings1\") \\\n",
    "    .merge(df_ratings_updated_temp.alias(\"ratings1_updated\"),\n",
    "           \"ratings1.Title = ratings1_updated.Title AND ratings1.Scores = ratings1_updated.Scores\") \\\n",
    "    .whenMatchedUpdate(set={\"Count\": \"ratings1_updated.Count\"}) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Save the merged data back to the original Delta table\n",
    "delta_table_ratings.toDF().write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/ratings_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = spark.read.format(\"delta\").load(\"/tmp/ratings_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating weighted values to get the final score of each book\n",
    "\n",
    "final_ratings = df_ratings.withColumn(\n",
    "    \"Final_score\", 0.04 * F.col(\"Count\") + 0.3 *2* F.col(\"Scores\") + 0.3 *10* F.col(\"Sentiments\")\n",
    ")\n",
    "final_ratings = final_ratings.na.drop(subset=[\"Final_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ratings = final_ratings.select(\"Title\", \"Final_score\").cache()\n",
    "#final_ratings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = df_books.withColumn(\"Categories\", F.translate(\"Categories\", \"[]'\", \"\"))\n",
    "df_books = df_books.withColumn(\"Categories\", F.split(F.col(\"Categories\"), \", \"))\n",
    "\n",
    "second_df = df_books.select(\"Title\", F.col(\"Categories\").getItem(0).alias(\"Category\"))\n",
    "\n",
    "# joining tables\n",
    "final_ratings = final_ratings.join(F.broadcast(second_df), on=\"Title\")\n",
    "# deleting duplicates\n",
    "final_ratings = final_ratings.dropDuplicates(['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = final_ratings.withColumn(\n",
    "    \"Title_final_score\", \n",
    "    F.concat(F.col(\"Title\"), F.lit(\" - \"), F.format_number(F.col(\"Final_score\"), 2))\n",
    ")\n",
    "merged_df.cache()\n",
    "final_ratings.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# making ranking\n",
    "window = Window.partitionBy('Category').orderBy(F.desc('Final_score'))\n",
    "\n",
    "# using extra counter column\n",
    "df = merged_df.withColumn('counter', F.row_number().over(window))\n",
    "merged_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoting\n",
    "pivot_df = df.groupBy('counter').pivot('Category').agg(F.first('Title_final_score'))\n",
    "pivot_df = pivot_df.drop('counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
